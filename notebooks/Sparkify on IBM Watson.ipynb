{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for a Spark session to start...\n",
      "Spark Initialization Done! ApplicationId = app-20191018123317-0000\n",
      "KERNEL_ID = 8fa3e70d-c376-4707-a5b6-5672aa777ad8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(artist='Martin Orford', auth='Logged In', firstName='Joseph', gender='M', itemInSession=20, lastName='Morales', length=597.55057, level='free', location='Corpus Christi, TX', method='PUT', page='NextSong', registration=1532063507000, sessionId=292, song='Grand Designs', status=200, ts=1538352011000, userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='293'),\n",
       " Row(artist=\"John Brown's Body\", auth='Logged In', firstName='Sawyer', gender='M', itemInSession=74, lastName='Larson', length=380.21179, level='free', location='Houston-The Woodlands-Sugar Land, TX', method='PUT', page='NextSong', registration=1538069638000, sessionId=97, song='Bulls', status=200, ts=1538352025000, userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', userId='98'),\n",
       " Row(artist='Afroman', auth='Logged In', firstName='Maverick', gender='M', itemInSession=184, lastName='Santiago', length=202.37016, level='paid', location='Orlando-Kissimmee-Sanford, FL', method='PUT', page='NextSong', registration=1535953455000, sessionId=178, song='Because I Got High', status=200, ts=1538352118000, userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', userId='179'),\n",
       " Row(artist=None, auth='Logged In', firstName='Maverick', gender='M', itemInSession=185, lastName='Santiago', length=None, level='paid', location='Orlando-Kissimmee-Sanford, FL', method='PUT', page='Logout', registration=1535953455000, sessionId=178, song=None, status=307, ts=1538352119000, userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', userId='179'),\n",
       " Row(artist='Lily Allen', auth='Logged In', firstName='Gianna', gender='F', itemInSession=22, lastName='Campos', length=194.53342, level='paid', location='Mobile, AL', method='PUT', page='NextSong', registration=1535931018000, sessionId=245, song='Smile (Radio Edit)', status=200, ts=1538352124000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='246')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ibmos2spark\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'endpoint': 'https://s3.eu-geo.objectstorage.service.networklayer.com',\n",
    "    'service_id': 'iam-ServiceId-7efbe802-cf47-4252-a5bb-dfc0e1b291fc',\n",
    "    'iam_service_endpoint': 'https://iam.eu-de.bluemix.net/oidc/token',\n",
    "    'api_key': 'j8ayzbE3wpe3Wt-0QLE2rFaSTqjnDolUo2vw7SWLNXP7'\n",
    "}\n",
    "\n",
    "configuration_name = 'os_ff46a710404449b7909df912ef988503_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# Since JSON data can be semi-structured and contain additional metadata, it is possible that you might face issues with the DataFrame layout.\n",
    "# Please read the documentation of 'SparkSession.read()' to learn more about the possibilities to adjust the data loading.\n",
    "# PySpark documentation: http://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.json\n",
    "\n",
    "df_data_1 = spark.read.json(cos.url('medium-sparkify-event-data.json', 'sparkify-donotdelete-pr-akwcx6xtarvcl3'))\n",
    "df_data_1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import RegexTokenizer, CountVectorizer, \\\n",
    "    IDF, StringIndexer, RegexTokenizer, VectorAssembler, Normalizer, StandardScaler\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType, StringType, FloatType\n",
    "from pyspark.ml.feature import RegexTokenizer, VectorAssembler, Normalizer, StandardScaler\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import desc, asc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql.functions import concat, lit, avg, split, isnan, isnull, when, count, col, \\\n",
    "sum, mean, stddev, min, max, countDistinct,approx_count_distinct, size, collect_set, round\n",
    "from pyspark.sql import Window\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, Normalizer, StandardScaler, MinMaxScaler\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, OneHotEncoderEstimator\n",
    "from pyspark.ml.classification import  RandomForestClassifier, LogisticRegression, LinearSVC, NaiveBayes, GBTClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning is done\n",
      "Feature creation is done\n",
      "Joining dataframes and feature extraction are done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(userId='100010', gender='F', churn=1, os_system='iPhone', location_first='CT', total_sessionId=2, total_itemInSession=137, time_after_id_creation(hour)=73.04, total_Top100_artist_alltime=18, total_Top100_song_week=12, total_thumbsup=4, total_thumbsdown=3, total_rolladvert=22, total_addfriend=3, total_addplaylist=1, total_sub_upgrade=0, total_sub_downgrade=0, total_error=0, total_logout=1, last_level='free', time_spent_Total_hour=7.01, time_spent_Total_day=0.29, time_after_id_creation(day)=3.04, avg_total_sessionId_afterCreation=0.66, avg_itemInSession_afterCreation=45.07, avg_thumbsup_afterCreation=1.32, avg_thumbsdown_afterCreation=0.99, avg_rolladvert_afterCreation=7.24, avg_addfriend_afterCreation=0.99, avg_addplaylist_afterCreation=0.33, avg_error_afterCreation=0.0, avg_logout_afterCreation=0.33, avg_total_Top100_artist_alltime=5.92, avg_total_Top100_song_week=3.95)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_dataframe_forML(df):\n",
    "    def subfunc_cleaning(df):\n",
    "        # Remove no id rows\n",
    "        df = df.filter(df[\"userId\"] != \"\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    def subfunc_featureCreation(df):\n",
    "        # Make a list of ids who churned\n",
    "        churn_id_list = df[df.page == \"Cancellation Confirmation\"].select(\"userId\").distinct().collect()\n",
    "        churn_id_list = [x['userId'] for x in churn_id_list]\n",
    "        create_churn_udf = udf(lambda userid: 1 if userid in churn_id_list else 0, IntegerType())\n",
    "        df = df.withColumn(\"churn\", create_churn_udf(df.userId))\n",
    "\n",
    "        # Create new userAgent column\n",
    "        def create_new_agent(userAgent):\n",
    "            if userAgent == None:\n",
    "                computer_os = None\n",
    "            else:\n",
    "                computer_os = userAgent.split()[1]\n",
    "                computer_os = re.sub(r'[\\W\\s]','' ,computer_os)\n",
    "\n",
    "            return  computer_os\n",
    "        create_new_agent_udf = udf(create_new_agent, StringType())\n",
    "        df = df.withColumn(\"os_system\", create_new_agent_udf(df.userAgent))\n",
    "\n",
    "        # Create new location column\n",
    "        def create_new_location(location):\n",
    "            if location == None:\n",
    "                location_first = None\n",
    "            else:\n",
    "                location_first = location.split(',')[-1].split('-')[0].strip()\n",
    "\n",
    "            return location_first\n",
    "\n",
    "        create_new_location_udf = udf(create_new_location, StringType())\n",
    "        df = df.withColumn(\"location_first\", create_new_location_udf(df.location))\n",
    "\n",
    "        # Create total number of sessionId column\n",
    "        w = Window.partitionBy(df.userId)\n",
    "        df = df.withColumn('total_sessionId', size(collect_set('sessionId').over(w)))\n",
    "\n",
    "        # Create total number of itemInSession column\n",
    "        df = df.withColumn('total_itemInSession', count('itemInSession').over(w))\n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "        # Create last_access_time,first_access_time columns\n",
    "        df = df.withColumn('last_access_time', max('ts').over(w))\n",
    "        df = df.withColumn('first_access_time', min('ts').over(w))\n",
    "\n",
    "        # Create last_level column\n",
    "        df = df.withColumn('last_level',when(df.last_access_time == df.ts, df.level))\n",
    "\n",
    "\n",
    "        # Create time difference column\n",
    "        def calculate_time_after_id_creation(last_time, first_time):\n",
    "            last_access_datetime = datetime.utcfromtimestamp(last_time / 1000)\n",
    "            first_access_datetime = datetime.utcfromtimestamp(first_time / 1000)\n",
    "            time_after_id_creation = last_access_datetime - first_access_datetime\n",
    "            result = time_after_id_creation.total_seconds()/3600\n",
    "\n",
    "            return result\n",
    "\n",
    "        calculate_time_after_id_creation_udf = udf(calculate_time_after_id_creation, FloatType())\n",
    "        df = df.withColumn(\"time_after_id_creation(hour)\", calculate_time_after_id_creation_udf(df.last_access_time, df.first_access_time))\n",
    "        df = df.withColumn(\"time_after_id_creation(hour)\", round(col('time_after_id_creation(hour)')/1, 2))\n",
    "\n",
    "\n",
    "        # convert timestamp to date (string)\n",
    "        def create_numWeek(ts):\n",
    "            return datetime.utcfromtimestamp(ts / 1000).strftime(\"%V\")\n",
    "        def create_numMonth(ts):\n",
    "            return datetime.utcfromtimestamp(ts / 1000).strftime(\"%m\")\n",
    "        def create_numYear(ts):\n",
    "            return datetime.utcfromtimestamp(ts / 1000).strftime(\"%Y\")\n",
    "\n",
    "        create_numWeek_udf = udf(create_numWeek, StringType())\n",
    "        create_numMonth_udf = udf(create_numMonth, StringType())\n",
    "        create_numYear_udf = udf(create_numYear, StringType())\n",
    "        df = df.withColumn('num_week', create_numWeek_udf(col('ts')))\n",
    "        df = df.withColumn('num_month', create_numMonth_udf(col('ts')))\n",
    "        df = df.withColumn('num_year', create_numYear_udf(col('ts')))\n",
    "\n",
    "        # Make a top_100 alltime artist list\n",
    "        tmp_list = df.where(df.artist != \"\").groupby(\"artist\").count().sort(col(\"count\").desc()).collect()[0:100]\n",
    "        top_100_alltime_artist_list = [row[\"artist\"] for row in tmp_list]\n",
    "        top_100_alltime_artist_list\n",
    "\n",
    "        # Make udf to set 1 at churn column in every row of the chrun users \n",
    "        def create_top100_alltime(artist):\n",
    "            if artist in top_100_alltime_artist_list:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "        create_top100_alltime_udf = udf(create_top100_alltime, IntegerType())\n",
    "        df = df.withColumn(\"top100_artist_alltime\", create_top100_alltime_udf(df.artist))\n",
    "        # Count total number of top 100\n",
    "        w = Window.partitionBy(df.userId)\n",
    "        df = df.withColumn('total_Top100_artist_alltime', sum('top100_artist_alltime').over(w))\n",
    "\n",
    "        # Make a dictionary of a best seller song list of each week\n",
    "        tmp_list = df.select(\"num_week\").distinct().sort(\"num_week\").collect()\n",
    "        available_week_list = [row[\"num_week\"] for row in tmp_list]\n",
    "        available_week_list\n",
    "\n",
    "\n",
    "    # Make a dictionary of a best seller song list of each week\n",
    "\n",
    "        def create_dict_top100_song_week(available_week_list):\n",
    "            dict_top100_song_week = dict()\n",
    "            for week in available_week_list:\n",
    "                top_100_week_song_list = df.where((df.artist != \"\") & (df.num_week == week)).groupby(\"song\",\"num_week\").count()\\\n",
    "                .sort(col(\"num_week\"), col(\"count\").desc()).collect()[0:100]\n",
    "                top_100_week_song_list = [row['song'] for row in top_100_week_song_list]\n",
    "                dict_top100_song_week[week] = top_100_week_song_list\n",
    "\n",
    "            return dict_top100_song_week\n",
    "\n",
    "        dict_top100_song_week = create_dict_top100_song_week(available_week_list)\n",
    "\n",
    "        # Make a top_100_song_week list\n",
    "        def create_top100_song_week(song, num_week):\n",
    "            if song in dict_top100_song_week[num_week]:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "        create_top100_song_week_udf = udf(create_top100_song_week, IntegerType())\n",
    "        df = df.withColumn(\"top100_song_week\", create_top100_song_week_udf(df.song, df.num_week))\n",
    "        # Count total number of top 100_song_week\n",
    "        w = Window.partitionBy(df.userId)\n",
    "        df = df.withColumn('total_Top100_song_week', sum('top100_song_week').over(w))\n",
    "        \n",
    "        \n",
    "        return df\n",
    "    \n",
    "\n",
    "\n",
    "    def subfunc_joinDf_featureExtraction(df):\n",
    "        # Create other dataframes to be joined to df dataframe to get \n",
    "        # Number of thumb up/ thumb down/ advert/ add friend/ upgrade/ downgrade/ error/ logout\n",
    "        thumbsup_df = df.where(df.page == 'Thumbs Up').groupBy(\"userId\").agg(count(\"page\").alias(\"total_thumbsup\"))\n",
    "        thumbsdown_df = df.where(df.page == 'Thumbs Down').groupBy(\"userId\").agg(count(\"page\").alias(\"total_thumbsdown\"))\n",
    "        advert_df =  df.where(df.page == 'Roll Advert').groupBy(\"userId\").agg(count(\"page\").alias(\"total_rolladvert\"))\n",
    "        addfriend_df =  df.where(df.page == 'Add Friend').groupBy(\"userId\").agg(count(\"page\").alias(\"total_addfriend\"))\n",
    "        addplaylist_df =  df.where(df.page == 'Add to Playlist').groupBy(\"userId\").agg(count(\"page\").alias(\"total_addplaylist\"))\n",
    "        sub_upgrade_df = df.where(df.page == 'Submit Upgrade').groupBy(\"userId\").agg(count(\"page\").alias(\"total_sub_upgrade\"))\n",
    "        sub_downgrade_df = df.where(df.page == 'Submit Downgrade').groupBy(\"userId\").agg(count(\"page\").alias(\"total_sub_downgrade\"))\n",
    "        error_df = df.where(df.page == 'Error').groupBy(\"userId\").agg(count(\"page\").alias(\"total_error\"))\n",
    "        logout_df = df.where(df.page == 'Logout').groupBy(\"userId\").agg(count(\"page\").alias(\"total_logout\"))\n",
    "        last_level_df = df[df.last_level != 'None'].select(\"userId\",\"last_level\")\n",
    "        \n",
    "        # Create total spent time\n",
    "        spent_time_df = df.groupBy(\"userId\", \"sessionId\").agg(max('ts').alias(\"max_ts_session\"), \n",
    "                                                              min('ts').alias(\"min_ts_session\")).orderBy(\"sessionId\", ascending=True)\n",
    "        def calculate_time_inSession(max_ts_session, min_ts_session):\n",
    "            max_ts_session_datetime = datetime.utcfromtimestamp(max_ts_session / 1000)\n",
    "            min_ts_session_datetime = datetime.utcfromtimestamp(min_ts_session / 1000)\n",
    "            spent_time_session = max_ts_session_datetime - min_ts_session_datetime\n",
    "            result = spent_time_session.total_seconds()/3600\n",
    "\n",
    "            return result\n",
    "\n",
    "        calculate_time_inSession_udf = udf(calculate_time_inSession, FloatType())\n",
    "        spent_time_df = spent_time_df.withColumn(\"time_spent_Session_hour\", \n",
    "                                                     calculate_time_inSession_udf(spent_time_df.max_ts_session, spent_time_df.min_ts_session))\n",
    "\n",
    "        w = Window.partitionBy(spent_time_df.userId)\n",
    "        spent_time_df = spent_time_df.withColumn(\"time_spent_Total_hour\", sum(spent_time_df.time_spent_Session_hour).over(w))\n",
    "        spent_time_df = spent_time_df.drop('max_ts_session','min_ts_session')\n",
    "        spent_time_df = spent_time_df.withColumn('time_spent_Session_hour', round(col('time_spent_Session_hour')/1, 2))\n",
    "        spent_time_df = spent_time_df.withColumn('time_spent_Total_hour', round(col('time_spent_Total_hour')/1, 2))\n",
    "        spent_time_df_only_total = spent_time_df.drop('time_spent_Session_hour','sessionId').distinct()\n",
    "        \n",
    "        # Make df_new for machine learning\n",
    "        df_new = df.select(\"userId\",'gender', 'churn', 'os_system', 'location_first', 'total_sessionId', 'total_itemInSession',\"time_after_id_creation(hour)\", \n",
    "                           \"total_Top100_artist_alltime\", \"total_Top100_song_week\").dropna().drop_duplicates()\n",
    "        df_new = df_new.join(thumbsup_df, 'userId', how='left').distinct()\n",
    "        df_new = df_new.join(thumbsdown_df, 'userId', how='left').distinct()\n",
    "        df_new = df_new.join(advert_df, 'userId', how='left').distinct()\n",
    "        df_new = df_new.join(addfriend_df, 'userId', how='left').distinct()\n",
    "        df_new = df_new.join(addplaylist_df, 'userId', how='left').distinct()\n",
    "        df_new = df_new.join(sub_upgrade_df, 'userId', how='left').distinct()\n",
    "        df_new = df_new.join(sub_downgrade_df, 'userId', how='left').distinct()\n",
    "        df_new = df_new.join(error_df, 'userId', how='left').distinct()\n",
    "        df_new = df_new.join(logout_df, 'userId', how='left').distinct()\n",
    "        df_new = df_new.fillna(0, subset=['total_thumbsup','total_thumbsdown', 'total_rolladvert', 'total_addfriend', 'total_addplaylist', 'total_sub_upgrade', \n",
    "                                         'total_sub_downgrade', 'total_error', 'total_logout'])\n",
    "        df_new = df_new.join(last_level_df, 'userId', how='left').distinct()\n",
    "        df_new = df_new.join(spent_time_df_only_total, 'userId', how='left').distinct()\n",
    "        \n",
    "        df_new1 = df_new.withColumn(\"time_spent_Total_day\", round(col('time_spent_Total_hour')/24, 2))\n",
    "        df_new1 = df_new1.withColumn(\"time_after_id_creation(day)\", round(col('time_after_id_creation(hour)')/24, 2))\n",
    "        df_new1 = df_new1.withColumn(\"avg_total_sessionId_afterCreation\", round(col('total_sessionId') / col('time_after_id_creation(day)'), 2))\n",
    "        df_new1 = df_new1.withColumn(\"avg_itemInSession_afterCreation\", round(col('total_itemInSession')/col('time_after_id_creation(day)'), 2))\n",
    "        df_new1 = df_new1.withColumn(\"avg_thumbsup_afterCreation\", round(col('total_thumbsup')/col('time_after_id_creation(day)'), 2))\n",
    "        df_new1 = df_new1.withColumn(\"avg_thumbsdown_afterCreation\", round((col('total_thumbsdown')/col('time_after_id_creation(day)')), 2))\n",
    "        df_new1 = df_new1.withColumn(\"avg_rolladvert_afterCreation\", round((col('total_rolladvert')/col('time_after_id_creation(day)')), 2))\n",
    "        df_new1 = df_new1.withColumn(\"avg_addfriend_afterCreation\", round((col('total_addfriend')/col('time_after_id_creation(day)')), 2))\n",
    "        df_new1 = df_new1.withColumn(\"avg_addplaylist_afterCreation\", round((col('total_addplaylist')/col('time_after_id_creation(day)')), 2))\n",
    "        df_new1 = df_new1.withColumn(\"avg_error_afterCreation\", round((col('total_error')/col('time_after_id_creation(day)')), 2))\n",
    "        df_new1 = df_new1.withColumn(\"avg_logout_afterCreation\", round((col('total_logout')/col('time_after_id_creation(day)')), 2))\n",
    "        df_new1 = df_new1.withColumn(\"avg_total_Top100_artist_alltime\", round((col('total_Top100_artist_alltime')/col('time_after_id_creation(day)')), 2))\n",
    "        df_new1 = df_new1.withColumn(\"avg_total_Top100_song_week\", round((col('total_Top100_song_week')/col('time_after_id_creation(day)')), 2))\n",
    "        \n",
    "        return df_new1\n",
    "    \n",
    "    df = subfunc_cleaning(df)\n",
    "    print(\"Cleaning is done\")\n",
    "    df = subfunc_featureCreation(df)\n",
    "    print(\"Feature creation is done\")\n",
    "    df_new1 = subfunc_joinDf_featureExtraction(df)\n",
    "    print(\"Joining dataframes and feature extraction are done\")\n",
    "    \n",
    "    return df_new1\n",
    "\n",
    "df_ML = create_dataframe_forML(df_data_1)\n",
    "df_ML.cache()\n",
    "df_ML.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "448"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ML.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId='100010', gender='F', churn=1, os_system='iPhone', location_first='CT', total_sessionId=2, total_itemInSession=137, time_after_id_creation(hour)=73.04, total_Top100_artist_alltime=18, total_Top100_song_week=12, total_thumbsup=4, total_thumbsdown=3, total_rolladvert=22, total_addfriend=3, total_addplaylist=1, total_sub_upgrade=0, total_sub_downgrade=0, total_error=0, total_logout=1, last_level='free', time_spent_Total_hour=7.01, time_spent_Total_day=0.29, time_after_id_creation(day)=3.04, avg_total_sessionId_afterCreation=0.66, avg_itemInSession_afterCreation=45.07, avg_thumbsup_afterCreation=1.32, avg_thumbsdown_afterCreation=0.99, avg_rolladvert_afterCreation=7.24, avg_addfriend_afterCreation=0.99, avg_addplaylist_afterCreation=0.33, avg_error_afterCreation=0.0, avg_logout_afterCreation=0.33, avg_total_Top100_artist_alltime=5.92, avg_total_Top100_song_week=3.95)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ML.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make train, test and validation sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298\n",
      "79\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "### Make train, test and validation sets \n",
    "train, test, validation = df_ML.randomSplit([0.7, 0.15, 0.15], seed = 44)\n",
    "print(train.count())\n",
    "print(test.count())\n",
    "print(validation.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index and encode categorical features gender, level and state\n",
    "stringIndexer_forGender = StringIndexer(inputCol=\"gender\", outputCol=\"indexed_gender\", handleInvalid = 'skip')\n",
    "stringIndexer_forLast_level = StringIndexer(inputCol=\"last_level\", outputCol=\"indexed_last_level\", handleInvalid = 'skip')\n",
    "stringIndexer_forLocation = StringIndexer(inputCol=\"location_first\", outputCol=\"indexed_location\", handleInvalid = 'skip')\n",
    "\n",
    "encoder = OneHotEncoderEstimator(inputCols=[\"indexed_gender\", \"indexed_last_level\", \"indexed_location\"],\n",
    "                                       outputCols=[\"gender_feat\", \"last_level_feat\", \"location_feat\"],\n",
    "                                handleInvalid = 'keep')\n",
    "\n",
    "# create vector for features\n",
    "features = ['gender_feat', 'last_level_feat', 'location_feat', 'time_after_id_creation(day)','avg_total_sessionId_afterCreation','avg_itemInSession_afterCreation',\n",
    "            'avg_thumbsup_afterCreation','avg_thumbsdown_afterCreation','avg_rolladvert_afterCreation','avg_addfriend_afterCreation',\n",
    "            'avg_addplaylist_afterCreation','avg_error_afterCreation','avg_logout_afterCreation','avg_total_Top100_artist_alltime','avg_total_Top100_song_week']\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"assembled_features\")\n",
    "scaler = MinMaxScaler(inputCol=\"assembled_features\" , outputCol=\"scaled_features\")\n",
    "\n",
    "# initialize random forest classifier\n",
    "rf = RandomForestClassifier(featuresCol=\"scaled_features\",labelCol=\"churn\")\n",
    "# initialize logistic regression\n",
    "lr = LogisticRegression(maxIter=5,threshold=0.3)\n",
    "# initialize N GBTClassifier\n",
    "gbt = GBTClassifier(featuresCol=\"scaled_features\",labelCol=\"churn\", maxIter=5, maxDepth=3)\n",
    "\n",
    "# assemble pipelines\n",
    "pipeline_rf = Pipeline(stages = [stringIndexer_forGender, stringIndexer_forLast_level, stringIndexer_forLocation, encoder, assembler, scaler ,rf])\n",
    "pipeline_lr = Pipeline(stages = [stringIndexer_forGender, stringIndexer_forLast_level, stringIndexer_forLocation, encoder, assembler, scaler ,lr])\n",
    "pipeline_gbt =  Pipeline(stages = [stringIndexer_forGender, stringIndexer_forLast_level, stringIndexer_forLocation, encoder, assembler, scaler, gbt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of training set:  0.912751677852349\n",
      "Precision of training set:  0.9361702127659575\n",
      "Recall of training set:  0.6567164179104478\n",
      "[[228.   3.]\n",
      " [ 23.  44.]]\n",
      "\n",
      "F1 score of test set:  0.8082191780821918\n",
      "Precision of test set:  0.75\n",
      "Recall of test set:  0.3333333333333333\n",
      "[[53.  2.]\n",
      " [12.  6.]]\n",
      "0:01:51.494839\n",
      "\n",
      "F1 score of validation set:  0.835820895522388\n",
      "Precision of validation set:  0.625\n",
      "Recall of validation set:  0.38461538461538464\n",
      "[[51.  3.]\n",
      " [ 8.  5.]]\n",
      "0:02:22.723863\n"
     ]
    }
   ],
   "source": [
    "# random forest\n",
    "##\n",
    "starttime = datetime.now()\n",
    "model_rf = pipeline_rf.fit(train)\n",
    "pred_train_rf = model_rf.transform(train)\n",
    "pred_test_rf = model_rf.transform(test)\n",
    "pred_validation_rf = model_rf.transform(validation)\n",
    "\n",
    "## train_set\n",
    "predictionAndLabels_rf = pred_train_rf.rdd.map(lambda x: (float(x.prediction), float(x.churn)))\n",
    "# Instantiate metrics object\n",
    "metrics_rf = MulticlassMetrics(predictionAndLabels_rf )\n",
    "# F1 score\n",
    "print(\"F1 score of training set: \", metrics_rf.fMeasure())\n",
    "print(\"Precision of training set: \", metrics_rf.precision(1))\n",
    "print(\"Recall of training set: \" , metrics_rf.recall(1))\n",
    "print(metrics_rf.confusionMatrix().toArray())\n",
    "print()\n",
    "\n",
    "## test_set\n",
    "predictionAndLabels_rf = pred_test_rf.rdd.map(lambda x: (float(x.prediction), float(x.churn)))\n",
    "# Instantiate metrics object\n",
    "metrics_rf = MulticlassMetrics(predictionAndLabels_rf)\n",
    "# F1 score\n",
    "print(\"F1 score of test set: \", metrics_rf.fMeasure())\n",
    "print(\"Precision of test set: \", metrics_rf.precision(1))\n",
    "print(\"Recall of test set: \" , metrics_rf.recall(1))\n",
    "print(metrics_rf.confusionMatrix().toArray())\n",
    "print(datetime.now() - starttime)\n",
    "print()\n",
    "\n",
    "## validation_set\n",
    "predictionAndLabels_rf = pred_validation_rf.rdd.map(lambda x: (float(x.prediction), float(x.churn)))\n",
    "# Instantiate metrics object\n",
    "metrics_rf = MulticlassMetrics(predictionAndLabels_rf)\n",
    "# F1 score\n",
    "print(\"F1 score of validation set: \", metrics_rf.fMeasure())\n",
    "print(\"Precision of validation set: \", metrics_rf.precision(1))\n",
    "print(\"Recall of validation set: \", metrics_rf.recall(1))\n",
    "print(metrics_rf.confusionMatrix().toArray())\n",
    "print(datetime.now() - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Logistic regression\n",
    "# ##\n",
    "# starttime = datetime.now()\n",
    "# model_lr = pipeline_lr.fit(train)\n",
    "# pred_train_lr = model_lr.transform(train)\n",
    "# pred_test_lr = model_lr.transform(test)\n",
    "# pred_validation_lr = model_lr.transform(validation)\n",
    "\n",
    "# ## train_set\n",
    "# predictionAndLabels_lr = pred_train_lr.rdd.map(lambda x: (float(x.prediction), float(x.churn)))\n",
    "# # Instantiate metrics object\n",
    "# metrics_lr = MulticlassMetrics(predictionAndLabels_lr)\n",
    "\n",
    "# # F1 score\n",
    "# print(\"F1 score of training set: \", metrics_lr.fMeasure())\n",
    "# print(\"Precision of training set: \", metrics_lr.precision(1))\n",
    "# print(\"Recall of training set: \" , metrics_lr.recall(1))\n",
    "# print(metrics_lr.confusionMatrix().toArray())\n",
    "# print()\n",
    "\n",
    "# ## test_set\n",
    "# predictionAndLabels_lr = pred_test_lr.rdd.map(lambda x: (float(x.prediction), float(x.churn)))\n",
    "# # Instantiate metrics object\n",
    "# metrics_lr = MulticlassMetrics(predictionAndLabels_lr)\n",
    "# # F1 score\n",
    "# print(\"F1 score of test set: \", metrics_lr.fMeasure())\n",
    "# print(\"Precision of test set: \", metrics_lr.precision(1))\n",
    "# print(\"Recall of test set: \", metrics_lr.recall(1))\n",
    "# print(metrics_lr.confusionMatrix().toArray())\n",
    "# print(datetime.now() - starttime)\n",
    "# print()\n",
    "\n",
    "# ## validation_set\n",
    "# predictionAndLabels_lr = pred_validation_lr.rdd.map(lambda x: (float(x.prediction), float(x.churn)))\n",
    "# # Instantiate metrics object\n",
    "# metrics_lr = MulticlassMetrics(predictionAndLabels_lr)\n",
    "# # F1 score\n",
    "# print(\"F1 score of validation set: \" , metrics_lr.fMeasure())\n",
    "# print(\"Precision of validation set:  \" , metrics_lr.precision(1))\n",
    "# print(\"Recall on validation set: \", metrics_lr.recall(1))\n",
    "# print(metrics_lr.confusionMatrix().toArray())\n",
    "# print(datetime.now() - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of training set:  0.9295302013422819\n",
      "Precision of training set:  0.896551724137931\n",
      "Recall of training set:  0.7761194029850746\n",
      "[[225.   6.]\n",
      " [ 15.  52.]]\n",
      "\n",
      "F1 score of test set:  0.863013698630137\n",
      "Precision of test set:  0.8333333333333334\n",
      "Recall of test set:  0.5555555555555556\n",
      "[[53.  2.]\n",
      " [ 8. 10.]]\n",
      "0:02:19.446047\n",
      "\n",
      "F1 score of validation set:  0.8208955223880597\n",
      "Precision of validation set:  0.5454545454545454\n",
      "Recall of validation set:  0.46153846153846156\n",
      "[[49.  5.]\n",
      " [ 7.  6.]]\n",
      "0:02:49.543222\n"
     ]
    }
   ],
   "source": [
    "# GBT\n",
    "##\n",
    "starttime = datetime.now()\n",
    "model_gbt = pipeline_gbt.fit(train)\n",
    "pred_train_gbt = model_gbt.transform(train)\n",
    "pred_test_gbt = model_gbt.transform(test)\n",
    "pred_validation_gbt = model_gbt.transform(validation)\n",
    "\n",
    "## train_set\n",
    "predictionAndLabels_gbt = pred_train_gbt.rdd.map(lambda x: (float(x.prediction), float(x.churn)))\n",
    "# Instantiate metrics object\n",
    "metrics_gbt = MulticlassMetrics(predictionAndLabels_gbt)\n",
    "\n",
    "# F1 score\n",
    "print(\"F1 score of training set: \", metrics_gbt.fMeasure())\n",
    "print(\"Precision of training set: \", metrics_gbt.precision(1))\n",
    "print(\"Recall of training set: \", metrics_gbt.recall(1))\n",
    "print(metrics_gbt.confusionMatrix().toArray())\n",
    "print()\n",
    "\n",
    "## test_set\n",
    "predictionAndLabels_gbt = pred_test_gbt.rdd.map(lambda x: (float(x.prediction), float(x.churn)))\n",
    "# Instantiate metrics object\n",
    "metrics_gbt = MulticlassMetrics(predictionAndLabels_gbt)\n",
    "# F1 score\n",
    "print(\"F1 score of test set: \", metrics_gbt.fMeasure())\n",
    "print(\"Precision of test set: \" , metrics_gbt.precision(1))\n",
    "print(\"Recall of test set: \" , metrics_gbt.recall(1))\n",
    "print(metrics_gbt.confusionMatrix().toArray())\n",
    "print(datetime.now() - starttime)\n",
    "print()\n",
    "\n",
    "## valid_set\n",
    "predictionAndLabels_gbt = pred_validation_gbt.rdd.map(lambda x: (float(x.prediction), float(x.churn)))\n",
    "# Instantiate metrics object\n",
    "metrics_gbt = MulticlassMetrics(predictionAndLabels_gbt)\n",
    "# F1 score\n",
    "print(\"F1 score of validation set: \", metrics_gbt.fMeasure())\n",
    "print(\"Precision of validation set: \", metrics_gbt.precision(1))\n",
    "print(\"Recall of validation set: \", metrics_gbt.recall(1))\n",
    "print(metrics_gbt.confusionMatrix().toArray())\n",
    "print(datetime.now() - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 with Spark",
   "language": "python3",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
