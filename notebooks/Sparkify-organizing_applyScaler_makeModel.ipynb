{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import RegexTokenizer, CountVectorizer, \\\n",
    "    IDF, StringIndexer, RegexTokenizer, VectorAssembler, Normalizer, StandardScaler\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType, StringType, FloatType\n",
    "from pyspark.ml.feature import RegexTokenizer, VectorAssembler, Normalizer, StandardScaler\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import desc, asc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql.functions import concat, lit, avg, split, isnan, isnull, when, count, col, \\\n",
    "sum, mean, stddev, min, max, countDistinct,approx_count_distinct, size, collect_set, round\n",
    "from pyspark.sql import Window\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, Normalizer, StandardScaler, MinMaxScaler\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, OneHotEncoderEstimator\n",
    "from pyspark.ml.classification import  RandomForestClassifier, LogisticRegression, LinearSVC, NaiveBayes, GBTClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"SmallSparkify\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a function for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning is done\n",
      "Feature creation is done\n",
      "Joining dataframes and feature extraction are done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(userId='100010', gender='F', churn=0, os_system='iPhone', location_first='CT', total_sessionId=7, total_itemInSession=381, time_after_id_creation(hour)=1061.23, total_Top100_artist_alltime=61, total_Top100_song_week=31, total_thumbsup=17, total_thumbsdown=5, total_rolladvert=52, total_addfriend=4, total_addplaylist=7, total_sub_upgrade=0, total_sub_downgrade=0, total_error=0, total_logout=5, last_level='free', time_spent_Total_hour=18.02, time_spent_Total_day=0.75, time_after_id_creation(day)=44.22, avg_total_sessionId_afterCreation=0.16, avg_itemInSession_afterCreation=8.62, avg_thumbsup_afterCreation=0.38, avg_thumbsdown_afterCreation=0.11, avg_rolladvert_afterCreation=1.18, avg_addfriend_afterCreation=0.09, avg_addplaylist_afterCreation=0.16, avg_error_afterCreation=0.0, avg_logout_afterCreation=0.11, avg_total_Top100_artist_alltime=1.38, avg_total_Top100_song_week=0.7)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_dataframe_forML(df):\n",
    "    def subfunc_cleaning(df):\n",
    "        # Remove no id rows\n",
    "        df = df.filter(df[\"userId\"] != \"\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    def subfunc_featureCreation(df):\n",
    "        # Make a list of ids who churned\n",
    "        churn_id_list = df[df.page == \"Cancellation Confirmation\"].select(\"userId\").distinct().collect()\n",
    "        churn_id_list = [x['userId'] for x in churn_id_list]\n",
    "        create_churn_udf = udf(lambda userid: 1 if userid in churn_id_list else 0, IntegerType())\n",
    "        df = df.withColumn(\"churn\", create_churn_udf(df.userId))\n",
    "\n",
    "        # Create new userAgent column\n",
    "        def create_new_agent(userAgent):\n",
    "            if userAgent == None:\n",
    "                computer_os = None\n",
    "            else:\n",
    "                computer_os = userAgent.split()[1]\n",
    "                computer_os = re.sub(r'[\\W\\s]','' ,computer_os)\n",
    "\n",
    "            return  computer_os\n",
    "        create_new_agent_udf = udf(create_new_agent, StringType())\n",
    "        df = df.withColumn(\"os_system\", create_new_agent_udf(df.userAgent))\n",
    "\n",
    "        # Create new location column\n",
    "        def create_new_location(location):\n",
    "            if location == None:\n",
    "                location_first = None\n",
    "            else:\n",
    "                location_first = location.split(',')[-1].split('-')[0].strip()\n",
    "\n",
    "            return location_first\n",
    "\n",
    "        create_new_location_udf = udf(create_new_location, StringType())\n",
    "        df = df.withColumn(\"location_first\", create_new_location_udf(df.location))\n",
    "\n",
    "        # Create total number of sessionId column\n",
    "        w = Window.partitionBy(df.userId)\n",
    "        df = df.withColumn('total_sessionId', size(collect_set('sessionId').over(w)))\n",
    "\n",
    "        # Create total number of itemInSession column\n",
    "        df = df.withColumn('total_itemInSession', count('itemInSession').over(w))\n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "        # Create last_access_time,first_access_time columns\n",
    "        df = df.withColumn('last_access_time', max('ts').over(w))\n",
    "        df = df.withColumn('first_access_time', min('ts').over(w))\n",
    "\n",
    "        # Create last_level column\n",
    "        df = df.withColumn('last_level',when(df.last_access_time == df.ts, df.level))\n",
    "\n",
    "\n",
    "        # Create time difference column\n",
    "        def calculate_time_after_id_creation(last_time, first_time):\n",
    "            last_access_datetime = datetime.utcfromtimestamp(last_time / 1000)\n",
    "            first_access_datetime = datetime.utcfromtimestamp(first_time / 1000)\n",
    "            time_after_id_creation = last_access_datetime - first_access_datetime\n",
    "            result = time_after_id_creation.total_seconds()/3600\n",
    "\n",
    "            return result\n",
    "\n",
    "        calculate_time_after_id_creation_udf = udf(calculate_time_after_id_creation, FloatType())\n",
    "        df = df.withColumn(\"time_after_id_creation(hour)\", calculate_time_after_id_creation_udf(df.last_access_time, df.first_access_time))\n",
    "        df = df.withColumn(\"time_after_id_creation(hour)\", round(col('time_after_id_creation(hour)')/1, 2))\n",
    "\n",
    "\n",
    "        # convert timestamp to date (string)\n",
    "        def create_numWeek(ts):\n",
    "            return datetime.utcfromtimestamp(ts / 1000).strftime(\"%V\")\n",
    "        def create_numMonth(ts):\n",
    "            return datetime.utcfromtimestamp(ts / 1000).strftime(\"%m\")\n",
    "        def create_numYear(ts):\n",
    "            return datetime.utcfromtimestamp(ts / 1000).strftime(\"%Y\")\n",
    "\n",
    "        create_numWeek_udf = udf(create_numWeek, StringType())\n",
    "        create_numMonth_udf = udf(create_numMonth, StringType())\n",
    "        create_numYear_udf = udf(create_numYear, StringType())\n",
    "        df = df.withColumn('num_week', create_numWeek_udf(col('ts')))\n",
    "        df = df.withColumn('num_month', create_numMonth_udf(col('ts')))\n",
    "        df = df.withColumn('num_year', create_numYear_udf(col('ts')))\n",
    "\n",
    "        # Make a top_100 alltime artist list\n",
    "        tmp_list = df.where(df.artist != \"\").groupby(\"artist\").count().sort(col(\"count\").desc()).collect()[0:100]\n",
    "        top_100_alltime_artist_list = [row[\"artist\"] for row in tmp_list]\n",
    "        top_100_alltime_artist_list\n",
    "\n",
    "        # Make udf to set 1 at churn column in every row of the chrun users \n",
    "        def create_top100_alltime(artist):\n",
    "            if artist in top_100_alltime_artist_list:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "        create_top100_alltime_udf = udf(create_top100_alltime, IntegerType())\n",
    "        df = df.withColumn(\"top100_artist_alltime\", create_top100_alltime_udf(df.artist))\n",
    "        # Count total number of top 100\n",
    "        w = Window.partitionBy(df.userId)\n",
    "        df = df.withColumn('total_Top100_artist_alltime', sum('top100_artist_alltime').over(w))\n",
    "\n",
    "        # Make a dictionary of a best seller song list of each week\n",
    "        tmp_list = df.select(\"num_week\").distinct().sort(\"num_week\").collect()\n",
    "        available_week_list = [row[\"num_week\"] for row in tmp_list]\n",
    "        available_week_list\n",
    "\n",
    "\n",
    "    # Make a dictionary of a best seller song list of each week\n",
    "\n",
    "        def create_dict_top100_song_week(available_week_list):\n",
    "            dict_top100_song_week = dict()\n",
    "            for week in available_week_list:\n",
    "                top_100_week_song_list = df.where((df.artist != \"\") & (df.num_week == week)).groupby(\"song\",\"num_week\").count()\\\n",
    "                .sort(col(\"num_week\"), col(\"count\").desc()).collect()[0:100]\n",
    "                top_100_week_song_list = [row['song'] for row in top_100_week_song_list]\n",
    "                dict_top100_song_week[week] = top_100_week_song_list\n",
    "\n",
    "            return dict_top100_song_week\n",
    "\n",
    "        dict_top100_song_week = create_dict_top100_song_week(available_week_list)\n",
    "\n",
    "        # Make a top_100_song_week list\n",
    "        def create_top100_song_week(song, num_week):\n",
    "            if song in dict_top100_song_week[num_week]:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "        create_top100_song_week_udf = udf(create_top100_song_week, IntegerType())\n",
    "        df = df.withColumn(\"top100_song_week\", create_top100_song_week_udf(df.song, df.num_week))\n",
    "        # Count total number of top 100_song_week\n",
    "        w = Window.partitionBy(df.userId)\n",
    "        df = df.withColumn('total_Top100_song_week', sum('top100_song_week').over(w))\n",
    "        \n",
    "        \n",
    "        return df\n",
    "    \n",
    "\n",
    "\n",
    "    def subfunc_joinDf_featureExtraction(df):\n",
    "        # Create other dataframes to be joined to df dataframe to get \n",
    "        # Number of thumb up/ thumb down/ advert/ add friend/ upgrade/ downgrade/ error/ logout\n",
    "        thumbsup_df = df.where(df.page == 'Thumbs Up').groupBy(\"userId\").agg(count(\"page\").alias(\"total_thumbsup\"))\n",
    "        thumbsdown_df = df.where(df.page == 'Thumbs Down').groupBy(\"userId\").agg(count(\"page\").alias(\"total_thumbsdown\"))\n",
    "        advert_df =  df.where(df.page == 'Roll Advert').groupBy(\"userId\").agg(count(\"page\").alias(\"total_rolladvert\"))\n",
    "        addfriend_df =  df.where(df.page == 'Add Friend').groupBy(\"userId\").agg(count(\"page\").alias(\"total_addfriend\"))\n",
    "        addplaylist_df =  df.where(df.page == 'Add to Playlist').groupBy(\"userId\").agg(count(\"page\").alias(\"total_addplaylist\"))\n",
    "        sub_upgrade_df = df.where(df.page == 'Submit Upgrade').groupBy(\"userId\").agg(count(\"page\").alias(\"total_sub_upgrade\"))\n",
    "        sub_downgrade_df = df.where(df.page == 'Submit Downgrade').groupBy(\"userId\").agg(count(\"page\").alias(\"total_sub_downgrade\"))\n",
    "        error_df = df.where(df.page == 'Error').groupBy(\"userId\").agg(count(\"page\").alias(\"total_error\"))\n",
    "        logout_df = df.where(df.page == 'Logout').groupBy(\"userId\").agg(count(\"page\").alias(\"total_logout\"))\n",
    "        last_level_df = df[df.last_level != 'None'].select(\"userId\",\"last_level\")\n",
    "        \n",
    "        # Create total spent time\n",
    "        spent_time_df = df.groupBy(\"userId\", \"sessionId\").agg(max('ts').alias(\"max_ts_session\"), \n",
    "                                                              min('ts').alias(\"min_ts_session\")).orderBy(\"sessionId\", ascending=True)\n",
    "        def calculate_time_inSession(max_ts_session, min_ts_session):\n",
    "            max_ts_session_datetime = datetime.utcfromtimestamp(max_ts_session / 1000)\n",
    "            min_ts_session_datetime = datetime.utcfromtimestamp(min_ts_session / 1000)\n",
    "            spent_time_session = max_ts_session_datetime - min_ts_session_datetime\n",
    "            result = spent_time_session.total_seconds()/3600\n",
    "\n",
    "            return result\n",
    "\n",
    "        calculate_time_inSession_udf = udf(calculate_time_inSession, FloatType())\n",
    "        spent_time_df = spent_time_df.withColumn(\"time_spent_Session_hour\", \n",
    "                                                     calculate_time_inSession_udf(spent_time_df.max_ts_session, spent_time_df.min_ts_session))\n",
    "\n",
    "        w = Window.partitionBy(spent_time_df.userId)\n",
    "        spent_time_df = spent_time_df.withColumn(\"time_spent_Total_hour\", sum(spent_time_df.time_spent_Session_hour).over(w))\n",
    "        spent_time_df = spent_time_df.drop('max_ts_session','min_ts_session')\n",
    "        spent_time_df = spent_time_df.withColumn('time_spent_Session_hour', round(col('time_spent_Session_hour')/1, 2))\n",
    "        spent_time_df = spent_time_df.withColumn('time_spent_Total_hour', round(col('time_spent_Total_hour')/1, 2))\n",
    "        spent_time_df_only_total = spent_time_df.drop('time_spent_Session_hour','sessionId').distinct()\n",
    "        \n",
    "        # Make df_new for machine learning\n",
    "        df_new = df.select(\"userId\",'gender', 'churn', 'os_system', 'location_first', 'total_sessionId', 'total_itemInSession',\"time_after_id_creation(hour)\", \n",
    "                           \"total_Top100_artist_alltime\", \"total_Top100_song_week\").dropna().drop_duplicates()\n",
    "        df_new = df_new.join(thumbsup_df, 'userId', how='left').distinct()\n",
    "        df_new = df_new.join(thumbsdown_df, 'userId', how='left').distinct()\n",
    "        df_new = df_new.join(advert_df, 'userId', how='left').distinct()\n",
    "        df_new = df_new.join(addfriend_df, 'userId', how='left').distinct()\n",
    "        df_new = df_new.join(addplaylist_df, 'userId', how='left').distinct()\n",
    "        df_new = df_new.join(sub_upgrade_df, 'userId', how='left').distinct()\n",
    "        df_new = df_new.join(sub_downgrade_df, 'userId', how='left').distinct()\n",
    "        df_new = df_new.join(error_df, 'userId', how='left').distinct()\n",
    "        df_new = df_new.join(logout_df, 'userId', how='left').distinct()\n",
    "        df_new = df_new.fillna(0, subset=['total_thumbsup','total_thumbsdown', 'total_rolladvert', 'total_addfriend', 'total_addplaylist', 'total_sub_upgrade', \n",
    "                                         'total_sub_downgrade', 'total_error', 'total_logout'])\n",
    "        df_new = df_new.join(last_level_df, 'userId', how='left').distinct()\n",
    "        df_new = df_new.join(spent_time_df_only_total, 'userId', how='left').distinct()\n",
    "        \n",
    "        df_new1 = df_new.withColumn(\"time_spent_Total_day\", round(col('time_spent_Total_hour')/24, 2))\n",
    "        df_new1 = df_new1.withColumn(\"time_after_id_creation(day)\", round(col('time_after_id_creation(hour)')/24, 2))\n",
    "        df_new1 = df_new1.withColumn(\"avg_total_sessionId_afterCreation\", round(col('total_sessionId') / col('time_after_id_creation(day)'), 2))\n",
    "        df_new1 = df_new1.withColumn(\"avg_itemInSession_afterCreation\", round(col('total_itemInSession')/col('time_after_id_creation(day)'), 2))\n",
    "        df_new1 = df_new1.withColumn(\"avg_thumbsup_afterCreation\", round(col('total_thumbsup')/col('time_after_id_creation(day)'), 2))\n",
    "        df_new1 = df_new1.withColumn(\"avg_thumbsdown_afterCreation\", round((col('total_thumbsdown')/col('time_after_id_creation(day)')), 2))\n",
    "        df_new1 = df_new1.withColumn(\"avg_rolladvert_afterCreation\", round((col('total_rolladvert')/col('time_after_id_creation(day)')), 2))\n",
    "        df_new1 = df_new1.withColumn(\"avg_addfriend_afterCreation\", round((col('total_addfriend')/col('time_after_id_creation(day)')), 2))\n",
    "        df_new1 = df_new1.withColumn(\"avg_addplaylist_afterCreation\", round((col('total_addplaylist')/col('time_after_id_creation(day)')), 2))\n",
    "        df_new1 = df_new1.withColumn(\"avg_error_afterCreation\", round((col('total_error')/col('time_after_id_creation(day)')), 2))\n",
    "        df_new1 = df_new1.withColumn(\"avg_logout_afterCreation\", round((col('total_logout')/col('time_after_id_creation(day)')), 2))\n",
    "        df_new1 = df_new1.withColumn(\"avg_total_Top100_artist_alltime\", round((col('total_Top100_artist_alltime')/col('time_after_id_creation(day)')), 2))\n",
    "        df_new1 = df_new1.withColumn(\"avg_total_Top100_song_week\", round((col('total_Top100_song_week')/col('time_after_id_creation(day)')), 2))\n",
    "\n",
    "        \n",
    "        return df_new1\n",
    "    \n",
    "    df = subfunc_cleaning(df)\n",
    "    print(\"Cleaning is done\")\n",
    "    df = subfunc_featureCreation(df)\n",
    "    print(\"Feature creation is done\")\n",
    "    df_new1 = subfunc_joinDf_featureExtraction(df)\n",
    "    print(\"Joining dataframes and feature extraction are done\")\n",
    "    \n",
    "    return df_new1\n",
    "\n",
    "df = spark.read.options(header=True).json(\"mini_sparkify_event_data.json\")\n",
    "df_ML = create_dataframe_forML(df)\n",
    "df_ML.cache()\n",
    "df_ML.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ML.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId='100010', gender='F', churn=0, os_system='iPhone', location_first='CT', total_sessionId=7, total_itemInSession=381, time_after_id_creation(hour)=1061.23, total_Top100_artist_alltime=61, total_Top100_song_week=31, total_thumbsup=17, total_thumbsdown=5, total_rolladvert=52, total_addfriend=4, total_addplaylist=7, total_sub_upgrade=0, total_sub_downgrade=0, total_error=0, total_logout=5, last_level='free', time_spent_Total_hour=18.02, time_spent_Total_day=0.75, time_after_id_creation(day)=44.22, avg_total_sessionId_afterCreation=0.16, avg_itemInSession_afterCreation=8.62, avg_thumbsup_afterCreation=0.38, avg_thumbsdown_afterCreation=0.11, avg_rolladvert_afterCreation=1.18, avg_addfriend_afterCreation=0.09, avg_addplaylist_afterCreation=0.16, avg_error_afterCreation=0.0, avg_logout_afterCreation=0.11, avg_total_Top100_artist_alltime=1.38, avg_total_Top100_song_week=0.7)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ML.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make train, test and validation sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make train, test and validation sets \n",
    "train, test, validation = df_ML.randomSplit([0.7, 0.15, 0.15], seed = 44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n",
      "38\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "print(train.count())\n",
    "print(test.count())\n",
    "print(validation.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId='100010', gender='F', churn=0, os_system='iPhone', location_first='CT', total_sessionId=7, total_itemInSession=381, time_after_id_creation(hour)=1061.23, total_Top100_artist_alltime=61, total_Top100_song_week=31, total_thumbsup=17, total_thumbsdown=5, total_rolladvert=52, total_addfriend=4, total_addplaylist=7, total_sub_upgrade=0, total_sub_downgrade=0, total_error=0, total_logout=5, last_level='free', time_spent_Total_hour=18.02, time_spent_Total_day=0.75, time_after_id_creation(day)=44.22, avg_total_sessionId_afterCreation=0.16, avg_itemInSession_afterCreation=8.62, avg_thumbsup_afterCreation=0.38, avg_thumbsdown_afterCreation=0.11, avg_rolladvert_afterCreation=1.18, avg_addfriend_afterCreation=0.09, avg_addplaylist_afterCreation=0.16, avg_error_afterCreation=0.0, avg_logout_afterCreation=0.11, avg_total_Top100_artist_alltime=1.38, avg_total_Top100_song_week=0.7)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ML.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Pipelines to feed data into classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(userId='100010', gender='F', churn=0, os_system='iPhone', location_first='CT', total_sessionId=7, total_itemInSession=381, time_after_id_creation(hour)=1061.23, total_Top100_artist_alltime=61, total_Top100_song_week=31, total_thumbsup=17, total_thumbsdown=5, total_rolladvert=52, total_addfriend=4, total_addplaylist=7, total_sub_upgrade=0, total_sub_downgrade=0, total_error=0, total_logout=5, last_level='free', time_spent_Total_hour=18.02, time_spent_Total_day=0.75, time_after_id_creation(day)=44.22, avg_total_sessionId_afterCreation=0.16, avg_itemInSession_afterCreation=8.62, avg_thumbsup_afterCreation=0.38, avg_thumbsdown_afterCreation=0.11, avg_rolladvert_afterCreation=1.18, avg_addfriend_afterCreation=0.09, avg_addplaylist_afterCreation=0.16, avg_error_afterCreation=0.0, avg_logout_afterCreation=0.11, avg_total_Top100_artist_alltime=1.38, avg_total_Top100_song_week=0.7, indexed_gender=1.0, indexed_last_level=1.0, indexed_location_first=9.0, gender_feat=SparseVector(2, {1: 1.0}), last_level_feat=SparseVector(2, {1: 1.0}), location_feat=SparseVector(41, {9: 1.0}), scaled_features=SparseVector(57, {1: 1.0, 3: 1.0, 13: 1.0, 45: 44.22, 46: 0.16, 47: 8.62, 48: 0.38, 49: 0.11, 50: 1.18, 51: 0.09, 52: 0.16, 54: 0.11, 55: 1.38, 56: 0.7}))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_for_stringindexer_list = [\"gender\", 'last_level', 'location_first']\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=\"indexed_\"+column) for column in column_for_stringindexer_list]\n",
    "encoder = OneHotEncoderEstimator(inputCols=[\"indexed_gender\", \"indexed_last_level\", \"indexed_location_first\"],\n",
    "                                       outputCols=[\"gender_feat\", \"last_level_feat\", \"location_feat\"],\n",
    "                                handleInvalid = 'keep')\n",
    "features = ['gender_feat', 'last_level_feat', 'location_feat', 'time_after_id_creation(day)','avg_total_sessionId_afterCreation','avg_itemInSession_afterCreation',\n",
    "            'avg_thumbsup_afterCreation','avg_thumbsdown_afterCreation','avg_rolladvert_afterCreation','avg_addfriend_afterCreation',\n",
    "            'avg_addplaylist_afterCreation','avg_error_afterCreation','avg_logout_afterCreation','avg_total_Top100_artist_alltime','avg_total_Top100_song_week']\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"scaled_features\")\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[indexers[0], indexers[1], indexers[2], encoder, assembler])\n",
    "df_ML1 = pipeline.fit(df_ML).transform(df_ML)\n",
    "df_ML1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=0, features=SparseVector(57, {1: 1.0, 3: 1.0, 13: 1.0, 45: 44.22, 46: 0.16, 47: 8.62, 48: 0.38, 49: 0.11, 50: 1.18, 51: 0.09, 52: 0.16, 54: 0.11, 55: 1.38, 56: 0.7}))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ML1_togo = df_ML1.select(col(\"churn\").alias(\"label\"), col(\"scaled_features\").alias(\"features\"))\n",
    "df_ML1_togo.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make train, test and validation sets \n",
    "train, test, validation = df_ML1_togo.randomSplit([0.7, 0.15, 0.15], seed = 41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize random forest classifier\n",
    "rf = RandomForestClassifier(featuresCol=\"features\",labelCol=\"label\")\n",
    "# initialize logistic regression\n",
    "lr = LogisticRegression(maxIter=5,threshold=0.3)\n",
    "# initialize logistic regression\n",
    "l_SVC = LinearSVC(featuresCol=\"features\",labelCol=\"label\",threshold=0.4)\n",
    "# initialize Naive Bayes\n",
    "nb = NaiveBayes(featuresCol=\"features\",labelCol=\"label\")\n",
    "# initialize N GBTClassifier\n",
    "gbt = GBTClassifier(featuresCol=\"features\",labelCol=\"label\", maxIter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of training set:  0.9294871794871795\n",
      "Precision of training set:  0.9666666666666667\n",
      "Recall of training set:  0.7435897435897436\n",
      "[[ 116.    1.]\n",
      " [  10.   29.]]\n",
      "\n",
      "F1 score of test set:  0.9166666666666666\n",
      "Precision of test set:  1.0\n",
      "Recall of test set:  0.6666666666666666\n",
      "[[ 27.   0.]\n",
      " [  3.   6.]]\n",
      "0:03:16.522448\n",
      "\n",
      "F1 score of validation set:  0.8484848484848485\n",
      "Precision of validation set:  0.3333333333333333\n",
      "Recall of validation set:  0.25\n",
      "[[ 27.   2.]\n",
      " [  3.   1.]]\n",
      "0:04:24.809489\n"
     ]
    }
   ],
   "source": [
    "# random forest\n",
    "##\n",
    "starttime = datetime.now()\n",
    "model_rf = rf.fit(train)\n",
    "pred_train_rf = model_rf.transform(train)\n",
    "pred_test_rf = model_rf.transform(test)\n",
    "pred_validation_rf = model_rf.transform(validation)\n",
    "\n",
    "## train_set\n",
    "predictionAndLabels_rf = pred_train_rf.rdd.map(lambda x: (float(x.prediction), float(x.label)))\n",
    "# Instantiate metrics object\n",
    "metrics_rf = MulticlassMetrics(predictionAndLabels_rf )\n",
    "# F1 score\n",
    "print(\"F1 score of training set: \", metrics_rf.fMeasure())\n",
    "print(\"Precision of training set: \", metrics_rf.precision(1))\n",
    "print(\"Recall of training set: \" , metrics_rf.recall(1))\n",
    "print(metrics_rf.confusionMatrix().toArray())\n",
    "print()\n",
    "\n",
    "## test_set\n",
    "predictionAndLabels_rf = pred_test_rf.rdd.map(lambda x: (float(x.prediction), float(x.label)))\n",
    "# Instantiate metrics object\n",
    "metrics_rf = MulticlassMetrics(predictionAndLabels_rf)\n",
    "# F1 score\n",
    "print(\"F1 score of test set: \", metrics_rf.fMeasure())\n",
    "print(\"Precision of test set: \", metrics_rf.precision(1))\n",
    "print(\"Recall of test set: \" , metrics_rf.recall(1))\n",
    "print(metrics_rf.confusionMatrix().toArray())\n",
    "print(datetime.now() - starttime)\n",
    "print()\n",
    "\n",
    "## validation_set\n",
    "predictionAndLabels_rf = pred_validation_rf.rdd.map(lambda x: (float(x.prediction), float(x.label)))\n",
    "# Instantiate metrics object\n",
    "metrics_rf = MulticlassMetrics(predictionAndLabels_rf)\n",
    "# F1 score\n",
    "print(\"F1 score of validation set: \", metrics_rf.fMeasure())\n",
    "print(\"Precision of validation set: \", metrics_rf.precision(1))\n",
    "print(\"Recall of validation set: \", metrics_rf.recall(1))\n",
    "print(metrics_rf.confusionMatrix().toArray())\n",
    "print(datetime.now() - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of training set:  0.8846153846153846\n",
      "Precision of training set:  0.7333333333333333\n",
      "Recall of training set:  0.8461538461538461\n",
      "[[ 105.   12.]\n",
      " [   6.   33.]]\n",
      "\n",
      "F1 score of test set:  0.6388888888888888\n",
      "Precision of test set:  0.3333333333333333\n",
      "Recall of test set:  0.4444444444444444\n",
      "[[ 19.   8.]\n",
      " [  5.   4.]]\n",
      "0:02:38.314477\n",
      "\n",
      "F1 score of validation set:  0.7575757575757576\n",
      "Precision of validation set:   0.16666666666666666\n",
      "Recall on validation set:  0.25\n",
      "[[ 24.   5.]\n",
      " [  3.   1.]]\n",
      "0:03:43.606134\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression\n",
    "##\n",
    "starttime = datetime.now()\n",
    "model_lr = lr.fit(train)\n",
    "pred_train_lr = model_lr.transform(train)\n",
    "pred_test_lr = model_lr.transform(test)\n",
    "pred_validation_lr = model_lr.transform(validation)\n",
    "\n",
    "## train_set\n",
    "predictionAndLabels_lr = pred_train_lr.rdd.map(lambda x: (float(x.prediction), float(x.label)))\n",
    "# Instantiate metrics object\n",
    "metrics_lr = MulticlassMetrics(predictionAndLabels_lr)\n",
    "\n",
    "# F1 score\n",
    "print(\"F1 score of training set: \", metrics_lr.fMeasure())\n",
    "print(\"Precision of training set: \", metrics_lr.precision(1))\n",
    "print(\"Recall of training set: \" , metrics_lr.recall(1))\n",
    "print(metrics_lr.confusionMatrix().toArray())\n",
    "print()\n",
    "\n",
    "## test_set\n",
    "predictionAndLabels_lr = pred_test_lr.rdd.map(lambda x: (float(x.prediction), float(x.label)))\n",
    "# Instantiate metrics object\n",
    "metrics_lr = MulticlassMetrics(predictionAndLabels_lr)\n",
    "# F1 score\n",
    "print(\"F1 score of test set: \", metrics_lr.fMeasure())\n",
    "print(\"Precision of test set: \", metrics_lr.precision(1))\n",
    "print(\"Recall of test set: \", metrics_lr.recall(1))\n",
    "print(metrics_lr.confusionMatrix().toArray())\n",
    "print(datetime.now() - starttime)\n",
    "print()\n",
    "\n",
    "## validation_set\n",
    "predictionAndLabels_lr = pred_validation_lr.rdd.map(lambda x: (float(x.prediction), float(x.label)))\n",
    "# Instantiate metrics object\n",
    "metrics_lr = MulticlassMetrics(predictionAndLabels_lr)\n",
    "# F1 score\n",
    "print(\"F1 score of validation set: \" , metrics_lr.fMeasure())\n",
    "print(\"Precision of validation set:  \" , metrics_lr.precision(1))\n",
    "print(\"Recall on validation set: \", metrics_lr.recall(1))\n",
    "print(metrics_lr.confusionMatrix().toArray())\n",
    "print(datetime.now() - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score on training dataset is 0.8910256410256411\n",
      "Precision on training dataset is 0.8928571428571429\n",
      "Recall on training dataset is 0.6410256410256411\n",
      "[[ 114.    3.]\n",
      " [  14.   25.]]\n",
      "\n",
      "F1 score on test dataset is 0.75\n",
      "Precision on test dataset is 0.5\n",
      "Recall on test dataset is 0.3333333333333333\n",
      "[[ 24.   3.]\n",
      " [  6.   3.]]\n",
      "0:19:42.210861\n"
     ]
    }
   ],
   "source": [
    "# Linear SVC\n",
    "##\n",
    "starttime = datetime.now()\n",
    "model_l_SVC = l_SVC.fit(train)\n",
    "pred_train_l_SVC = model_l_SVC.transform(train)\n",
    "pred_test_l_SVC = model_l_SVC.transform(test)\n",
    "\n",
    "## train_set\n",
    "predictionAndLabels_l_SVC = pred_train_l_SVC.rdd.map(lambda x: (float(x.prediction), float(x.label)))\n",
    "# Instantiate metrics object\n",
    "metrics_l_SVC = MulticlassMetrics(predictionAndLabels_l_SVC)\n",
    "\n",
    "# F1 score\n",
    "print(\"F1 score on training dataset is %s\" % metrics_l_SVC.fMeasure())\n",
    "print(\"Precision on training dataset is %s\" % metrics_l_SVC.precision(1))\n",
    "print(\"Recall on training dataset is %s\" % metrics_l_SVC.recall(1))\n",
    "print(metrics_l_SVC.confusionMatrix().toArray())\n",
    "print()\n",
    "\n",
    "## test_set\n",
    "predictionAndLabels_l_SVC = pred_test_l_SVC.rdd.map(lambda x: (float(x.prediction), float(x.label)))\n",
    "# Instantiate metrics object\n",
    "metrics_l_SVC = MulticlassMetrics(predictionAndLabels_l_SVC)\n",
    "# F1 score\n",
    "print(\"F1 score on test dataset is %s\" % metrics_l_SVC.fMeasure())\n",
    "print(\"Precision on test dataset is %s\" % metrics_l_SVC.precision(1))\n",
    "print(\"Recall on test dataset is %s\" % metrics_l_SVC.recall(1))\n",
    "print(metrics_l_SVC.confusionMatrix().toArray())\n",
    "print(datetime.now() - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of training set:  0.8012820512820513\n",
      "Precision of training set:  0.7\n",
      "Recall of training set:  0.358974358974359\n",
      "[[ 111.    6.]\n",
      " [  25.   14.]]\n",
      "\n",
      "F1 score of test set:  0.7777777777777778\n",
      "Precision of test set:  0.5714285714285714\n",
      "Recall of test set:  0.4444444444444444\n",
      "[[ 24.   3.]\n",
      " [  5.   4.]]\n",
      "0:02:16.665492\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "##\n",
    "starttime = datetime.now()\n",
    "model_nb = nb.fit(train)\n",
    "pred_train_nb = model_nb.transform(train)\n",
    "pred_test_nb = model_nb.transform(test)\n",
    "\n",
    "## train_set\n",
    "predictionAndLabels_nb = pred_train_nb.rdd.map(lambda x: (float(x.prediction), float(x.label)))\n",
    "# Instantiate metrics object\n",
    "metrics_nb = MulticlassMetrics(predictionAndLabels_nb)\n",
    "\n",
    "# F1 score\n",
    "print(\"F1 score of training set: \" , metrics_nb.fMeasure())\n",
    "print(\"Precision of training set: \" , metrics_nb.precision(1))\n",
    "print(\"Recall of training set: \" , metrics_nb.recall(1))\n",
    "print(metrics_nb.confusionMatrix().toArray())\n",
    "print()\n",
    "\n",
    "## test_set\n",
    "predictionAndLabels_nb = pred_test_nb.rdd.map(lambda x: (float(x.prediction), float(x.label)))\n",
    "# Instantiate metrics object\n",
    "metrics_nb = MulticlassMetrics(predictionAndLabels_nb)\n",
    "# F1 score\n",
    "print(\"F1 score of test set: \" , metrics_nb.fMeasure())\n",
    "print(\"Precision of test set: \" , metrics_nb.precision(1))\n",
    "print(\"Recall of test set: \" , metrics_nb.recall(1))\n",
    "print(metrics_nb.confusionMatrix().toArray())\n",
    "print(datetime.now() - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of training set:  1.0\n",
      "Precision of training set:  1.0\n",
      "Recall of training set:  1.0\n",
      "[[ 117.    0.]\n",
      " [   0.   39.]]\n",
      "\n",
      "F1 score of test set:  0.8055555555555556\n",
      "Precision of test set:  0.5833333333333334\n",
      "Recall of test set:  0.7777777777777778\n",
      "[[ 22.   5.]\n",
      " [  2.   7.]]\n",
      "0:04:22.122882\n",
      "\n",
      "F1 score of validation set:  0.9090909090909091\n",
      "Precision of validation set:  0.6666666666666666\n",
      "Recall of validation set:  0.5\n",
      "[[ 28.   1.]\n",
      " [  2.   2.]]\n",
      "0:05:27.898781\n"
     ]
    }
   ],
   "source": [
    "# GBT\n",
    "##\n",
    "starttime = datetime.now()\n",
    "model_gbt = gbt.fit(train)\n",
    "pred_train_gbt = model_gbt.transform(train)\n",
    "pred_test_gbt = model_gbt.transform(test)\n",
    "pred_validation_gbt = model_gbt.transform(validation)\n",
    "\n",
    "## train_set\n",
    "predictionAndLabels_gbt = pred_train_gbt.rdd.map(lambda x: (float(x.prediction), float(x.label)))\n",
    "# Instantiate metrics object\n",
    "metrics_gbt = MulticlassMetrics(predictionAndLabels_gbt)\n",
    "\n",
    "# F1 score\n",
    "print(\"F1 score of training set: \", metrics_gbt.fMeasure())\n",
    "print(\"Precision of training set: \", metrics_gbt.precision(1))\n",
    "print(\"Recall of training set: \", metrics_gbt.recall(1))\n",
    "print(metrics_gbt.confusionMatrix().toArray())\n",
    "print()\n",
    "\n",
    "## test_set\n",
    "predictionAndLabels_gbt = pred_test_gbt.rdd.map(lambda x: (float(x.prediction), float(x.label)))\n",
    "# Instantiate metrics object\n",
    "metrics_gbt = MulticlassMetrics(predictionAndLabels_gbt)\n",
    "# F1 score\n",
    "print(\"F1 score of test set: \", metrics_gbt.fMeasure())\n",
    "print(\"Precision of test set: \" , metrics_gbt.precision(1))\n",
    "print(\"Recall of test set: \" , metrics_gbt.recall(1))\n",
    "print(metrics_gbt.confusionMatrix().toArray())\n",
    "print(datetime.now() - starttime)\n",
    "print()\n",
    "\n",
    "## valid_set\n",
    "predictionAndLabels_gbt = pred_validation_gbt.rdd.map(lambda x: (float(x.prediction), float(x.label)))\n",
    "# Instantiate metrics object\n",
    "metrics_gbt = MulticlassMetrics(predictionAndLabels_gbt)\n",
    "# F1 score\n",
    "print(\"F1 score of validation set: \", metrics_gbt.fMeasure())\n",
    "print(\"Precision of validation set: \", metrics_gbt.precision(1))\n",
    "print(\"Recall of validation set: \", metrics_gbt.recall(1))\n",
    "print(metrics_gbt.confusionMatrix().toArray())\n",
    "print(datetime.now() - starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the results above, only 3 classifiers including `Random Forest`, `Logistic Regression`, and `GBT classifier` passed to the final steps. The reasons of disqualification of the rest two models are that LinearSVC was too slow and Naive Bayes did not have good F1 score. Logistic Regression did not show good F1 score, but I passed it to the final steps because the model has threshold parameter that I expecedt that it might help to increase precision and recall values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines after MinMax Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(userId='100010', gender='F', churn=0, os_system='iPhone', location_first='CT', total_sessionId=7, total_itemInSession=381, time_after_id_creation(hour)=1061.23, total_Top100_artist_alltime=61, total_Top100_song_week=31, total_thumbsup=17, total_thumbsdown=5, total_rolladvert=52, total_addfriend=4, total_addplaylist=7, total_sub_upgrade=0, total_sub_downgrade=0, total_error=0, total_logout=5, last_level='free', time_spent_Total_hour=18.02, time_spent_Total_day=0.75, time_after_id_creation(day)=44.22, avg_total_sessionId_afterCreation=0.16, avg_itemInSession_afterCreation=8.62, avg_thumbsup_afterCreation=0.38, avg_thumbsdown_afterCreation=0.11, avg_rolladvert_afterCreation=1.18, avg_addfriend_afterCreation=0.09, avg_addplaylist_afterCreation=0.16, avg_error_afterCreation=0.0, avg_logout_afterCreation=0.11, avg_total_Top100_artist_alltime=1.38, avg_total_Top100_song_week=0.7, indexed_gender=1.0, indexed_last_level=1.0, indexed_location_first=9.0, gender_feat=SparseVector(2, {1: 1.0}), last_level_feat=SparseVector(2, {1: 1.0}), location_feat=SparseVector(41, {9: 1.0}), scaled_features=SparseVector(57, {1: 1.0, 3: 1.0, 13: 1.0, 45: 44.22, 46: 0.16, 47: 8.62, 48: 0.38, 49: 0.11, 50: 1.18, 51: 0.09, 52: 0.16, 54: 0.11, 55: 1.38, 56: 0.7}), ScaledFeatures=DenseVector([0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7249, 0.0011, 0.01, 0.0046, 0.0033, 0.0118, 0.0016, 0.0024, 0.0, 0.0066, 0.0109, 0.0109]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_for_stringindexer_list = [\"gender\", 'last_level', 'location_first']\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=\"indexed_\"+column) for column in column_for_stringindexer_list]\n",
    "encoder = OneHotEncoderEstimator(inputCols=[\"indexed_gender\", \"indexed_last_level\", \"indexed_location_first\"],\n",
    "                                       outputCols=[\"gender_feat\", \"last_level_feat\", \"location_feat\"],\n",
    "                                handleInvalid = 'keep')\n",
    "features = ['gender_feat', 'last_level_feat', 'location_feat', 'time_after_id_creation(day)','avg_total_sessionId_afterCreation','avg_itemInSession_afterCreation',\n",
    "            'avg_thumbsup_afterCreation','avg_thumbsdown_afterCreation','avg_rolladvert_afterCreation','avg_addfriend_afterCreation',\n",
    "            'avg_addplaylist_afterCreation','avg_error_afterCreation','avg_logout_afterCreation','avg_total_Top100_artist_alltime','avg_total_Top100_song_week']\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"scaled_features\")\n",
    "scaler = MinMaxScaler(inputCol=\"scaled_features\" , outputCol=\"ScaledFeatures\")\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[indexers[0], indexers[1], indexers[2], encoder, assembler, scaler])\n",
    "df_ML2= pipeline.fit(df_ML).transform(df_ML)\n",
    "df_ML2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=0, features=SparseVector(57, {1: 1.0, 3: 1.0, 13: 1.0, 45: 44.22, 46: 0.16, 47: 8.62, 48: 0.38, 49: 0.11, 50: 1.18, 51: 0.09, 52: 0.16, 54: 0.11, 55: 1.38, 56: 0.7}))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ML2_togo = df_ML2.select(col(\"churn\").alias(\"label\"), col(\"scaled_features\").alias(\"features\"))\n",
    "df_ML2_togo.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make train, test and validation sets \n",
    "train, test, validation = df_ML2_togo.randomSplit([0.7, 0.15, 0.15], seed = 41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize random forest classifier\n",
    "rf = RandomForestClassifier(featuresCol=\"features\",labelCol=\"label\")\n",
    "# initialize logistic regression\n",
    "lr = LogisticRegression(maxIter=5,threshold=0.3)\n",
    "# initialize N GBTClassifier\n",
    "gbt = GBTClassifier(featuresCol=\"features\",labelCol=\"label\", maxIter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of training set:  0.9294871794871795\n",
      "Precision of training set:  0.9666666666666667\n",
      "Recall of training set:  0.7435897435897436\n",
      "[[ 116.    1.]\n",
      " [  10.   29.]]\n",
      "\n",
      "F1 score of test set:  0.9166666666666666\n",
      "Precision of test set:  1.0\n",
      "Recall of test set:  0.6666666666666666\n",
      "[[ 27.   0.]\n",
      " [  3.   6.]]\n",
      "0:03:01.402539\n",
      "\n",
      "F1 score of validation set:  0.8484848484848485\n",
      "Precision of validation set:  0.3333333333333333\n",
      "Recall of validation set:  0.25\n",
      "[[ 27.   2.]\n",
      " [  3.   1.]]\n",
      "0:04:09.096067\n"
     ]
    }
   ],
   "source": [
    "# random forest\n",
    "##\n",
    "starttime = datetime.now()\n",
    "model_rf = rf.fit(train)\n",
    "pred_train_rf = model_rf.transform(train)\n",
    "pred_test_rf = model_rf.transform(test)\n",
    "pred_validation_rf = model_rf.transform(validation)\n",
    "\n",
    "## train_set\n",
    "predictionAndLabels_rf = pred_train_rf.rdd.map(lambda x: (float(x.prediction), float(x.label)))\n",
    "# Instantiate metrics object\n",
    "metrics_rf = MulticlassMetrics(predictionAndLabels_rf )\n",
    "# F1 score\n",
    "print(\"F1 score of training set: \", metrics_rf.fMeasure())\n",
    "print(\"Precision of training set: \", metrics_rf.precision(1))\n",
    "print(\"Recall of training set: \" , metrics_rf.recall(1))\n",
    "print(metrics_rf.confusionMatrix().toArray())\n",
    "print()\n",
    "\n",
    "## test_set\n",
    "predictionAndLabels_rf = pred_test_rf.rdd.map(lambda x: (float(x.prediction), float(x.label)))\n",
    "# Instantiate metrics object\n",
    "metrics_rf = MulticlassMetrics(predictionAndLabels_rf)\n",
    "# F1 score\n",
    "print(\"F1 score of test set: \", metrics_rf.fMeasure())\n",
    "print(\"Precision of test set: \", metrics_rf.precision(1))\n",
    "print(\"Recall of test set: \" , metrics_rf.recall(1))\n",
    "print(metrics_rf.confusionMatrix().toArray())\n",
    "print(datetime.now() - starttime)\n",
    "print()\n",
    "\n",
    "## validation_set\n",
    "predictionAndLabels_rf = pred_validation_rf.rdd.map(lambda x: (float(x.prediction), float(x.label)))\n",
    "# Instantiate metrics object\n",
    "metrics_rf = MulticlassMetrics(predictionAndLabels_rf)\n",
    "# F1 score\n",
    "print(\"F1 score of validation set: \", metrics_rf.fMeasure())\n",
    "print(\"Precision of validation set: \", metrics_rf.precision(1))\n",
    "print(\"Recall of validation set: \", metrics_rf.recall(1))\n",
    "print(metrics_rf.confusionMatrix().toArray())\n",
    "print(datetime.now() - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of training set:  0.8846153846153846\n",
      "Precision of training set:  0.7333333333333333\n",
      "Recall of training set:  0.8461538461538461\n",
      "[[ 105.   12.]\n",
      " [   6.   33.]]\n",
      "\n",
      "F1 score of test set:  0.6388888888888888\n",
      "Precision of test set:  0.3333333333333333\n",
      "Recall of test set:  0.4444444444444444\n",
      "[[ 19.   8.]\n",
      " [  5.   4.]]\n",
      "0:02:35.169486\n",
      "\n",
      "F1 score of validation set:  0.7575757575757576\n",
      "Precision of validation set:   0.16666666666666666\n",
      "Recall on validation set:  0.25\n",
      "[[ 24.   5.]\n",
      " [  3.   1.]]\n",
      "0:03:40.341912\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression\n",
    "##\n",
    "starttime = datetime.now()\n",
    "model_lr = lr.fit(train)\n",
    "pred_train_lr = model_lr.transform(train)\n",
    "pred_test_lr = model_lr.transform(test)\n",
    "pred_validation_lr = model_lr.transform(validation)\n",
    "\n",
    "## train_set\n",
    "predictionAndLabels_lr = pred_train_lr.rdd.map(lambda x: (float(x.prediction), float(x.label)))\n",
    "# Instantiate metrics object\n",
    "metrics_lr = MulticlassMetrics(predictionAndLabels_lr)\n",
    "\n",
    "# F1 score\n",
    "print(\"F1 score of training set: \", metrics_lr.fMeasure())\n",
    "print(\"Precision of training set: \", metrics_lr.precision(1))\n",
    "print(\"Recall of training set: \" , metrics_lr.recall(1))\n",
    "print(metrics_lr.confusionMatrix().toArray())\n",
    "print()\n",
    "\n",
    "## test_set\n",
    "predictionAndLabels_lr = pred_test_lr.rdd.map(lambda x: (float(x.prediction), float(x.label)))\n",
    "# Instantiate metrics object\n",
    "metrics_lr = MulticlassMetrics(predictionAndLabels_lr)\n",
    "# F1 score\n",
    "print(\"F1 score of test set: \", metrics_lr.fMeasure())\n",
    "print(\"Precision of test set: \", metrics_lr.precision(1))\n",
    "print(\"Recall of test set: \", metrics_lr.recall(1))\n",
    "print(metrics_lr.confusionMatrix().toArray())\n",
    "print(datetime.now() - starttime)\n",
    "print()\n",
    "\n",
    "## validation_set\n",
    "predictionAndLabels_lr = pred_validation_lr.rdd.map(lambda x: (float(x.prediction), float(x.label)))\n",
    "# Instantiate metrics object\n",
    "metrics_lr = MulticlassMetrics(predictionAndLabels_lr)\n",
    "# F1 score\n",
    "print(\"F1 score of validation set: \" , metrics_lr.fMeasure())\n",
    "print(\"Precision of validation set:  \" , metrics_lr.precision(1))\n",
    "print(\"Recall on validation set: \", metrics_lr.recall(1))\n",
    "print(metrics_lr.confusionMatrix().toArray())\n",
    "print(datetime.now() - starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of training set:  1.0\n",
      "Precision of training set:  1.0\n",
      "Recall of training set:  1.0\n",
      "[[ 117.    0.]\n",
      " [   0.   39.]]\n",
      "\n",
      "F1 score of test set:  0.8055555555555556\n",
      "Precision of test set:  0.5833333333333334\n",
      "Recall of test set:  0.7777777777777778\n",
      "[[ 22.   5.]\n",
      " [  2.   7.]]\n",
      "0:04:19.272090\n",
      "\n",
      "F1 score of validation set:  0.9090909090909091\n",
      "Precision of validation set:  0.6666666666666666\n",
      "Recall of validation set:  0.5\n",
      "[[ 28.   1.]\n",
      " [  2.   2.]]\n",
      "0:05:25.579408\n"
     ]
    }
   ],
   "source": [
    "# GBT\n",
    "##\n",
    "starttime = datetime.now()\n",
    "model_gbt = gbt.fit(train)\n",
    "pred_train_gbt = model_gbt.transform(train)\n",
    "pred_test_gbt = model_gbt.transform(test)\n",
    "pred_validation_gbt = model_gbt.transform(validation)\n",
    "\n",
    "## train_set\n",
    "predictionAndLabels_gbt = pred_train_gbt.rdd.map(lambda x: (float(x.prediction), float(x.label)))\n",
    "# Instantiate metrics object\n",
    "metrics_gbt = MulticlassMetrics(predictionAndLabels_gbt)\n",
    "\n",
    "# F1 score\n",
    "print(\"F1 score of training set: \", metrics_gbt.fMeasure())\n",
    "print(\"Precision of training set: \", metrics_gbt.precision(1))\n",
    "print(\"Recall of training set: \", metrics_gbt.recall(1))\n",
    "print(metrics_gbt.confusionMatrix().toArray())\n",
    "print()\n",
    "\n",
    "## test_set\n",
    "predictionAndLabels_gbt = pred_test_gbt.rdd.map(lambda x: (float(x.prediction), float(x.label)))\n",
    "# Instantiate metrics object\n",
    "metrics_gbt = MulticlassMetrics(predictionAndLabels_gbt)\n",
    "# F1 score\n",
    "print(\"F1 score of test set: \", metrics_gbt.fMeasure())\n",
    "print(\"Precision of test set: \" , metrics_gbt.precision(1))\n",
    "print(\"Recall of test set: \" , metrics_gbt.recall(1))\n",
    "print(metrics_gbt.confusionMatrix().toArray())\n",
    "print(datetime.now() - starttime)\n",
    "print()\n",
    "\n",
    "## valid_set\n",
    "predictionAndLabels_gbt = pred_validation_gbt.rdd.map(lambda x: (float(x.prediction), float(x.label)))\n",
    "# Instantiate metrics object\n",
    "metrics_gbt = MulticlassMetrics(predictionAndLabels_gbt)\n",
    "# F1 score\n",
    "print(\"F1 score of validation set: \", metrics_gbt.fMeasure())\n",
    "print(\"Precision of validation set: \", metrics_gbt.precision(1))\n",
    "print(\"Recall of validation set: \", metrics_gbt.recall(1))\n",
    "print(metrics_gbt.confusionMatrix().toArray())\n",
    "print(datetime.now() - starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I chose `GBTclassifier` as the final model for this Sparkify project. \n",
    "\n",
    "To select the best model, I considered two metrics F1 score and recall. `Logistic Regression` was not qualified because I tested  several times with different thresholds, it could not show a good F1 score compared to other two. In terms of `F1 score`, `Random Forest Classifier` showed a good F1 score, a good precision, but a bad recall.  The main goal of this project is to predict Sparkify users who are going to churn for `Sparkify`. With a expected churn list, Sparkify will do something to prevent the users from churning. \n",
    "\n",
    "\n",
    "**So, it is meaningful to scarifice some F1 score and get higher recall for Sparkify because the company could do some promotions on the user who are reallly going to churn.** If I selected  `Random Forest Classifier` as my prediction model, the majority of the real churn users would not be predicted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
